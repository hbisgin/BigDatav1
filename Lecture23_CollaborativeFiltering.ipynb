{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83579b9-39d5-4318-b119-b9d3471f98a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Define the Coefficient Matrix 'a'\n",
    "# a represents the coefficients of the unknown variables (x1, x2)\n",
    "a = np.array([\n",
    "    [2, 3],\n",
    "    [1, 4]\n",
    "])\n",
    "\n",
    "# 2. Define the Result Vector 'b'\n",
    "# b represents the results of the equations\n",
    "b = np.array([12, 11])\n",
    "\n",
    "# 3. Use np.linalg.solve to find the unknown vector 'x'\n",
    "# x = [x1, x2]\n",
    "x = np.linalg.solve(a, b)\n",
    "\n",
    "print(\"--- Solving the System a*x = b ---\")\n",
    "print(f\"Coefficient Matrix (a):\\n{a}\")\n",
    "print(f\"\\nResult Vector (b):\\n{b}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Solution Vector (x1, x2):\\n{x}\")\n",
    "\n",
    "# 4. Verification (Optional but helpful)\n",
    "# Check if a * x equals b\n",
    "b_check = a @ x\n",
    "print(f\"\\nVerification (a * x):\\n{b_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac450f70-d717-4b3c-9f39-958d07e988e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def als_numpy(R, rank=2, max_iters=10, alpha=0.01, lambda_reg=0.1):\n",
    "    \"\"\"\n",
    "    Alternating Least Squares (ALS) implementation using NumPy.\n",
    "\n",
    "    R: The rating matrix (Users x Items). Missing values should be 0.\n",
    "    rank: The number of latent factors (k).\n",
    "    max_iters: Maximum number of iterations (epochs).\n",
    "    alpha: Learning rate (not strictly used here, but for context).\n",
    "    lambda_reg: Regularization parameter (lambda).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get dimensions\n",
    "    num_users, num_items = R.shape\n",
    "    \n",
    "    # 1. Initialize factor matrices (U: Users x Rank, V: Items x Rank)\n",
    "    # V is initialized as the transpose of the item factors (Items x Rank) for calculation ease.\n",
    "    U = np.random.rand(num_users, rank)\n",
    "    V = np.random.rand(num_items, rank)\n",
    "    \n",
    "    # Find observed ratings (where R > 0)\n",
    "    observed_mask = R > 0\n",
    "    \n",
    "    # 2. ALS Optimization Loop\n",
    "    for iteration in range(max_iters):\n",
    "        \n",
    "        # --- A. FIX V, SOLVE FOR U (User Factors) ---\n",
    "        for i in range(num_users):\n",
    "            # Items rated by user i\n",
    "            V_i = V[observed_mask[i, :]]\n",
    "            R_i = R[i, observed_mask[i, :]]\n",
    "            \n",
    "            # Least Squares Solution (Normal Equation):\n",
    "            # U_i = (V_i^T * V_i + lambda * I)^-1 * V_i^T * R_i\n",
    "            \n",
    "            # Identity matrix for regularization\n",
    "            reg_term = lambda_reg * np.eye(rank)\n",
    "            \n",
    "            # Solve for U[i, :] using matrix inversion\n",
    "            U[i, :] = np.linalg.solve(\n",
    "                V_i.T @ V_i + reg_term, \n",
    "                V_i.T @ R_i\n",
    "            )\n",
    "\n",
    "        # --- B. FIX U, SOLVE FOR V (Item Factors) ---\n",
    "        for j in range(num_items):\n",
    "            # Users who rated item j\n",
    "            U_j = U[observed_mask[:, j]]\n",
    "            R_j = R[observed_mask[:, j], j]\n",
    "            \n",
    "            # Solve for V[j, :] using matrix inversion\n",
    "            reg_term = lambda_reg * np.eye(rank)\n",
    "\n",
    "            V[j, :] = np.linalg.solve(\n",
    "                U_j.T @ U_j + reg_term, \n",
    "                U_j.T @ R_j\n",
    "            )\n",
    "        \n",
    "        # --- C. Check Loss (Optional but good for monitoring) ---\n",
    "        # Calculate predicted ratings\n",
    "        R_pred = U @ V.T\n",
    "        \n",
    "        # Calculate error only on observed ratings\n",
    "        error = R_pred[observed_mask] - R[observed_mask]\n",
    "        \n",
    "        # Calculate Regularized Squared Error (similar to Spark's RMSE goal)\n",
    "        loss = np.sum(error**2) + lambda_reg * (np.sum(U**2) + np.sum(V**2))\n",
    "        \n",
    "        # print(f\"Iteration {iteration+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "    return U, V, R_pred\n",
    "\n",
    "# --- RUN THE EXAMPLE ---\n",
    "\n",
    "# The toy rating matrix R (Users x Items)\n",
    "R = np.array([\n",
    "    [5., 3., 0., 1.],\n",
    "    [4., 0., 0., 1.],\n",
    "    [1., 1., 0., 5.],\n",
    "    [0., 0., 5., 4.],\n",
    "    [0., 1., 5., 4.]\n",
    "])\n",
    "\n",
    "# Run the ALS algorithm\n",
    "U_factors, V_factors, R_predicted = als_numpy(R, rank=2, max_iters=20, lambda_reg=0.1)\n",
    "\n",
    "print(\"--- Original Rating Matrix (R) ---\")\n",
    "print(R)\n",
    "print(\"\\n--- Predicted Rating Matrix (R_pred) ---\")\n",
    "# Round the prediction for readability\n",
    "print(np.round(R_predicted, 2))\n",
    "print(\"\\n--- Extracted User Factors (U) ---\")\n",
    "print(np.round(U_factors, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d73a7da-c799-4fe8-b872-86934c569a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fadab825-57cc-407e-b5a3-37e879237a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2c34b6-c9cd-4128-8d05-32f7aa8ab168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_raw = spark.read.csv(\n",
    "    \"dbfs:/FileStore/BDA_Datasets/sample_movielens_ratings.txt\", # Often a .dat extension for this format\n",
    "    header=False,\n",
    "    schema=movie_schema,\n",
    "    sep=\"::\"                     # Set the custom delimiter\n",
    ")\n",
    "\n",
    "# Select only the three columns required for ALS\n",
    "training_data = training_raw.select(\"user_id\", \"item_id\", \"rating\")\n",
    "training_data.cache() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d056e321-3ec3-476d-8daa-39adf8102c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a04178-4533-4290-a568-3ec60603ba33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/BDA_Datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6195b1b1-f7b0-486b-946c-66a458f8e9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Load the smaller testing data\n",
    "test_raw = spark.read.csv(\n",
    "    \"dbfs:/FileStore/BDA_Datasets/movielens_test.data\", # Use the same settings for the test file\n",
    "    header=False,\n",
    "    schema=movie_schema,\n",
    "    sep=\",\"\n",
    ")\n",
    "test_data = test_raw.select(\"user_id\", \"item_id\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed00f34-3a95-4afb-bcfb-31927be37d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabed925-e995-4b3c-a52d-852d8eba4775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# --- TRAINING THE MODEL ---\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=42\n",
    ")\n",
    "model = als.fit(training_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c4c9ad-7dda-4b73-96e8-ea00459dac71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1. Generate predictions on the training data\n",
    "predictions_train = model.transform(training_data)\n",
    "\n",
    "# 2. Define the evaluator (using the same evaluator defined for the test set)\n",
    "train_evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# 3. Calculate RMSE and drop rows where prediction is NULL (due to cold start)\n",
    "rmse_train = train_evaluator.evaluate(predictions_train.na.drop())\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE) on TRAINING data = {rmse_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd4c4e9-8f14-411a-8684-d8653b37da8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_train.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4004e2-3f2d-49b9-8102-b873b984c30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- EVALUATION ---\n",
    "\n",
    "predictions_test = model.transform(test_data)\n",
    "\n",
    "test_evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse_test = test_evaluator.evaluate(predictions_test.na.drop())\n",
    "print(f\"Root Mean Squared Error (RMSE) on TEST data = {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73956032-bee5-4d1d-9351-6f6aebee8e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_test.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23ffcb7e-5f30-4f65-ade1-70785748b191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's do some parameter search. I'm first going to split my training data into training and validation before actually going to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d8469a-e777-4acd-a585-5d2bc7be3562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Set the split ratios (e.g., 80% for training, 20% for validation)\n",
    "(train_set, validation_set) = training_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache the sets for faster iterative access during parameter testing\n",
    "train_set.cache()\n",
    "validation_set.cache()\n",
    "\n",
    "# Define the evaluator once\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8ad0bb9-40f6-436e-8ecd-ffd96cb76474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since cross validation could be a bit costly, this time around I made it simple in a way that we will go through a loop to test different parameters only on this 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d23941-8de9-4af7-a55a-0c5959ee4dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS #I keep adding those in case you want to run this independently\n",
    "\n",
    "# --- TRAINING THE MODEL ---\n",
    "# --- Parameters to Test ---\n",
    "# Define a list of (rank, regParam) tuples to try\n",
    "param_combinations = [\n",
    "    (5, 0.1),    # Rank=5, RegParam=0.1\n",
    "    (10, 0.1),   # Rank=10, RegParam=0.1\n",
    "    (10, 0.01),\n",
    "    (20, 0.1),\n",
    "    (20, 0.001)\n",
    "]\n",
    "best_rmse = float('inf')\n",
    "best_params = {}\n",
    "results = []\n",
    "\n",
    "print(\"Starting Hold-Out Validation...\")\n",
    "\n",
    "for current_rank, current_regParam in param_combinations:\n",
    "    \n",
    "    # 1. Initialize ALS with current parameters\n",
    "    als = ALS(\n",
    "        rank=current_rank,\n",
    "        maxIter=10,\n",
    "        regParam=current_regParam,\n",
    "        userCol=\"user_id\",\n",
    "        itemCol=\"item_id\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # 2. Train the model on the dedicated Training Set\n",
    "    model = als.fit(train_set)\n",
    "\n",
    "    # 3. Predict on the dedicated Validation Set\n",
    "    predictions = model.transform(validation_set)\n",
    "    \n",
    "    # 4. Evaluate RMSE on the validation set (ignoring NaNs from cold start)\n",
    "    rmse = evaluator.evaluate(predictions.na.drop())\n",
    "    \n",
    "    results.append({\n",
    "        'rank': current_rank,\n",
    "        'regParam': current_regParam,\n",
    "        'validation_rmse': rmse\n",
    "    })\n",
    "    \n",
    "    print(f\"Tested Rank={current_rank}, RegParam={current_regParam}: RMSE={rmse:.4f}\")\n",
    "\n",
    "    # 5. Track the best result\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = {\n",
    "            'rank': current_rank,\n",
    "            'regParam': current_regParam,\n",
    "            'maxIter': 10 # Fixed for this example\n",
    "        }\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Validation Complete. Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Best Parameters: Rank={best_params['rank']}, RegParam={best_params['regParam']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2e5c30-8f10-4efe-ae40-26834b32905d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrain the final model using the best parameters found\n",
    "final_als = ALS(\n",
    "    rank=best_params['rank'],\n",
    "    maxIter=10,\n",
    "    regParam=best_params['regParam'],\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "final_model = final_als.fit(training_data)\n",
    "\n",
    "print(\"\\nFinal ALS Model trained on all available training data using optimal parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "773d6d06-144b-43f5-b3c9-ddac3e8ee029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Do your test again on your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4722ec8b-5f3a-45ac-adb6-ccb12702626d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_test = final_model.transform(test_data)\n",
    "regressor_evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse_test = regressor_evaluator.evaluate(predictions_test.na.drop())\n",
    "print(f\"Root Mean Squared Error (RMSE) on TEST data = {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0782b3ff-a3e8-4d9f-9c31-cd4a8df84b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_test.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a93f9a0-7cef-47a2-9357-4b70fece0724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#ParamGrid & CrossValidation for future reference, but I'm not going to do this part in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d39db0c-f4ff-45c2-84d1-94d0a344474e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define the search space for hyperparameters\n",
    "# Note: You can expand these lists to search more combinations!\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [5, 10, 15]) \\ \n",
    "    .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(als.maxIter, [10]) \\\n",
    "    .build()\n",
    "\n",
    "# The grid above will test 3 ranks * 3 regParams * 1 maxIter = 9 total models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af4191c-c803-492f-b60c-a02771028881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is our CrossValidator like we did before\n",
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,    # K-Fold Cross-Validation (commonly 3 or 5)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Starting 5-Fold Cross-Validation...\")\n",
    "\n",
    "# 2. Run Cross-Validation (This is the time-consuming step!)\n",
    "# The CrossValidator will fit the total of 9 models * 5 folds = 45 ALS models.\n",
    "cv_model = cv.fit(training_data)\n",
    "\n",
    "print(\"Cross-Validation complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lecture23_CollaborativeFiltering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

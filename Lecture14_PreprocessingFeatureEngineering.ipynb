{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND7iYQGRL/GkBk4Nu1OlQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbisgin/BigDatav1/blob/main/Lecture14_PreprocessingFeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTINOUS FEATURES, NORMALIZATION, & MORE ON CATEGORICAL VARIABLES"
      ],
      "metadata": {
        "id": "o3yCY-98z5Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import (\n",
        "    Bucketizer, QuantileDiscretizer, StandardScaler,\n",
        "    MinMaxScaler, MaxAbsScaler, ElementwiseProduct,\n",
        "    Normalizer, StringIndexer, IndexToString\n",
        ")\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"ContinuousFeaturesExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "n6f0CHNq0Abx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Continuous Features Example\n"
      ],
      "metadata": {
        "id": "ZjVNJ8br0Llr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "contDF = spark.range(0, 20).selectExpr(\"cast(id as double)\")\n",
        "\n",
        "# Bucketing (manual splits)\n",
        "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
        "bucketer = Bucketizer(splits=bucketBorders, inputCol=\"id\", outputCol=\"bucketed_id\")\n",
        "bucketedDF = bucketer.transform(contDF)\n",
        "bucketedDF.show()\n",
        "\n",
        "# Bucketing with QuantileDiscretizer (automatic percentiles)\n",
        "quantBucketer = QuantileDiscretizer(numBuckets=5, inputCol=\"id\", outputCol=\"quantile_bucket\")\n",
        "fittedBucketer = quantBucketer.fit(contDF)\n",
        "quantiledDF = fittedBucketer.transform(contDF)\n",
        "quantiledDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny57N5A-0EUA",
        "outputId": "cd7b9171-ecc8-422c-deb0-38cd48824107"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+\n",
            "|  id|bucketed_id|\n",
            "+----+-----------+\n",
            "| 0.0|        0.0|\n",
            "| 1.0|        0.0|\n",
            "| 2.0|        0.0|\n",
            "| 3.0|        0.0|\n",
            "| 4.0|        0.0|\n",
            "| 5.0|        1.0|\n",
            "| 6.0|        1.0|\n",
            "| 7.0|        1.0|\n",
            "| 8.0|        1.0|\n",
            "| 9.0|        1.0|\n",
            "|10.0|        2.0|\n",
            "|11.0|        2.0|\n",
            "|12.0|        2.0|\n",
            "|13.0|        2.0|\n",
            "|14.0|        2.0|\n",
            "|15.0|        2.0|\n",
            "|16.0|        2.0|\n",
            "|17.0|        2.0|\n",
            "|18.0|        2.0|\n",
            "|19.0|        2.0|\n",
            "+----+-----------+\n",
            "\n",
            "+----+---------------+\n",
            "|  id|quantile_bucket|\n",
            "+----+---------------+\n",
            "| 0.0|            0.0|\n",
            "| 1.0|            0.0|\n",
            "| 2.0|            0.0|\n",
            "| 3.0|            1.0|\n",
            "| 4.0|            1.0|\n",
            "| 5.0|            1.0|\n",
            "| 6.0|            1.0|\n",
            "| 7.0|            2.0|\n",
            "| 8.0|            2.0|\n",
            "| 9.0|            2.0|\n",
            "|10.0|            2.0|\n",
            "|11.0|            3.0|\n",
            "|12.0|            3.0|\n",
            "|13.0|            3.0|\n",
            "|14.0|            3.0|\n",
            "|15.0|            4.0|\n",
            "|16.0|            4.0|\n",
            "|17.0|            4.0|\n",
            "|18.0|            4.0|\n",
            "|19.0|            4.0|\n",
            "+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Scaling and Normalization Example"
      ],
      "metadata": {
        "id": "_jZGPIxf0RY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(0, Vectors.dense([1.0, 0.1, -1.0])),\n",
        "        (1, Vectors.dense([3.0, 10.1, 3.0]))]\n",
        "scaleDF = spark.createDataFrame(data, [\"id\", \"features\"])\n",
        "\n",
        "# StandardScaler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "sScaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_std\", withMean=False, withStd=True)\n",
        "scaledDF = sScaler.fit(scaleDF).transform(scaleDF)\n",
        "scaledDF.show(truncate=False)\n",
        "\n",
        "# MinMaxScaler\n",
        "minMax = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_minmax\", min=0.0, max=1.0)\n",
        "minmaxDF = minMax.fit(scaleDF).transform(scaleDF)\n",
        "minmaxDF.show(truncate=False)\n",
        "\n",
        "# MaxAbsScaler\n",
        "maScaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaled_maxabs\")\n",
        "maxabsDF = maScaler.fit(scaleDF).transform(scaleDF)\n",
        "maxabsDF.show(truncate=False)\n",
        "\n",
        "# ElementwiseProduct\n",
        "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
        "scalingUp = ElementwiseProduct(scalingVec=scaleUpVec, inputCol=\"features\", outputCol=\"scaled_custom\")\n",
        "scaledCustomDF = scalingUp.transform(scaleDF)\n",
        "scaledCustomDF.show(truncate=False)\n",
        "\n",
        "# Normalizer (Manhattan norm p=1)\n",
        "manhattan = Normalizer(inputCol=\"features\", outputCol=\"norm_L1\", p=1.0)\n",
        "normalizedDF = manhattan.transform(scaleDF)\n",
        "normalizedDF.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd7TGKWq0dhD",
        "outputId": "840d8a1f-bdd1-4f20-8a8d-d8f77a5dc83f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+-------------------------------------------------------------+\n",
            "|id |features      |scaled_std                                                   |\n",
            "+---+--------------+-------------------------------------------------------------+\n",
            "|0  |[1.0,0.1,-1.0]|[0.7071067811865475,0.01414213562373095,-0.35355339059327373]|\n",
            "|1  |[3.0,10.1,3.0]|[2.1213203435596424,1.428355697996826,1.0606601717798212]    |\n",
            "+---+--------------+-------------------------------------------------------------+\n",
            "\n",
            "+---+--------------+-------------+\n",
            "|id |features      |scaled_minmax|\n",
            "+---+--------------+-------------+\n",
            "|0  |[1.0,0.1,-1.0]|(3,[],[])    |\n",
            "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]|\n",
            "+---+--------------+-------------+\n",
            "\n",
            "+---+--------------+-------------------------------------------------------------+\n",
            "|id |features      |scaled_maxabs                                                |\n",
            "+---+--------------+-------------------------------------------------------------+\n",
            "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009903,-0.3333333333333333]|\n",
            "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]                                                |\n",
            "+---+--------------+-------------------------------------------------------------+\n",
            "\n",
            "+---+--------------+-----------------+\n",
            "|id |features      |scaled_custom    |\n",
            "+---+--------------+-----------------+\n",
            "|0  |[1.0,0.1,-1.0]|[10.0,1.5,-20.0] |\n",
            "|1  |[3.0,10.1,3.0]|[30.0,151.5,60.0]|\n",
            "+---+--------------+-----------------+\n",
            "\n",
            "+---+--------------+---------------------------------------------------------------+\n",
            "|id |features      |norm_L1                                                        |\n",
            "+---+--------------+---------------------------------------------------------------+\n",
            "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
            "|1  |[3.0,10.1,3.0]|[0.18633540372670807,0.6273291925465838,0.18633540372670807]   |\n",
            "+---+--------------+---------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. StringIndexer + IndexToString Example\n"
      ],
      "metadata": {
        "id": "eN-9K0jx0jz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Sh1RRJ7ivS",
        "outputId": "b03fc5d0-6bed-403d-9901-095a8ac2aed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+\n",
            "|category|categoryIndex|\n",
            "+--------+-------------+\n",
            "|   apple|          1.0|\n",
            "|  banana|          0.0|\n",
            "|  orange|          2.0|\n",
            "|  banana|          0.0|\n",
            "+--------+-------------+\n",
            "\n",
            "+--------+-------------+----------------+\n",
            "|category|categoryIndex|originalCategory|\n",
            "+--------+-------------+----------------+\n",
            "|   apple|          1.0|           apple|\n",
            "|  banana|          0.0|          banana|\n",
            "|  orange|          2.0|          orange|\n",
            "|  banana|          0.0|          banana|\n",
            "+--------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "data = [Row(category=\"apple\"), Row(category=\"banana\"), Row(category=\"orange\"), Row(category=\"banana\")]\n",
        "catDF = spark.createDataFrame(data)\n",
        "\n",
        "# Convert text labels to indices\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "indexedDF = indexer.fit(catDF).transform(catDF)\n",
        "indexedDF.show()\n",
        "\n",
        "# Convert indices back to text labels\n",
        "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\",\n",
        "                          labels=indexer.fit(catDF).labels)\n",
        "convertedDF = converter.transform(indexedDF)\n",
        "convertedDF.show()\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Stop Spark\n",
        "# -----------------------------------------------\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT TRANSFORMATION + FEATURE SELECTION (Step-by-Step View)\n"
      ],
      "metadata": {
        "id": "Svy2wuzM0q9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, NGram,\n",
        "    CountVectorizer, ChiSqSelector\n",
        ")\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"TextFeatureSelectionSteps\").getOrCreate()"
      ],
      "metadata": {
        "id": "NxCa9bvc0usB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Sample dataset"
      ],
      "metadata": {
        "id": "eO_4EF1B0yKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (0, \"Big data analytics with Spark and Hadoop\"),\n",
        "    (1, \"Python for machine learning and data science\"),\n",
        "    (0, \"Big data tools like Spark, Hive, and HDFS\"),\n",
        "    (1, \"Deep learning using TensorFlow and PyTorch\"),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"label\", \"Description\"])\n",
        "print(\"=== Original Data ===\")\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej_ZDbfO0_7r",
        "outputId": "e7a9d53b-eeca-4071-abf1-3d9ae84a38fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Original Data ===\n",
            "+-----+--------------------------------------------+\n",
            "|label|Description                                 |\n",
            "+-----+--------------------------------------------+\n",
            "|0    |Big data analytics with Spark and Hadoop    |\n",
            "|1    |Python for machine learning and data science|\n",
            "|0    |Big data tools like Spark, Hive, and HDFS   |\n",
            "|1    |Deep learning using TensorFlow and PyTorch  |\n",
            "+-----+--------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization"
      ],
      "metadata": {
        "id": "VYcTlI6n1DKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"Description\", outputCol=\"tokens\")\n",
        "tokenized = tokenizer.transform(df)\n",
        "print(\"=== After Tokenization ===\")\n",
        "tokenized.select(\"Description\", \"tokens\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NheSN1AO1IMm",
        "outputId": "e3c80520-0dde-4a3c-8b79-e2b534d1961e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== After Tokenization ===\n",
            "+--------------------------------------------+----------------------------------------------------+\n",
            "|Description                                 |tokens                                              |\n",
            "+--------------------------------------------+----------------------------------------------------+\n",
            "|Big data analytics with Spark and Hadoop    |[big, data, analytics, with, spark, and, hadoop]    |\n",
            "|Python for machine learning and data science|[python, for, machine, learning, and, data, science]|\n",
            "|Big data tools like Spark, Hive, and HDFS   |[big, data, tools, like, spark,, hive,, and, hdfs]  |\n",
            "|Deep learning using TensorFlow and PyTorch  |[deep, learning, using, tensorflow, and, pytorch]   |\n",
            "+--------------------------------------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. StopWords Removal"
      ],
      "metadata": {
        "id": "FWgU-KYP1KUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "remover = StopWordsRemover(\n",
        "    inputCol=\"tokens\",\n",
        "    outputCol=\"filtered\",\n",
        "    stopWords=StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        ")\n",
        "filtered = remover.transform(tokenized)\n",
        "print(\"=== After StopWords Removal ===\")\n",
        "filtered.select(\"tokens\", \"filtered\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7HYF5GH1OD2",
        "outputId": "efc118fd-bb88-45fd-d278-22a37e4789c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== After StopWords Removal ===\n",
            "+----------------------------------------------------+---------------------------------------------+\n",
            "|tokens                                              |filtered                                     |\n",
            "+----------------------------------------------------+---------------------------------------------+\n",
            "|[big, data, analytics, with, spark, and, hadoop]    |[big, data, analytics, spark, hadoop]        |\n",
            "|[python, for, machine, learning, and, data, science]|[python, machine, learning, data, science]   |\n",
            "|[big, data, tools, like, spark,, hive,, and, hdfs]  |[big, data, tools, like, spark,, hive,, hdfs]|\n",
            "|[deep, learning, using, tensorflow, and, pytorch]   |[deep, learning, using, tensorflow, pytorch] |\n",
            "+----------------------------------------------------+---------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. N-Gram Creation\n"
      ],
      "metadata": {
        "id": "JlkcFjl01RY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
        "ngramDF = ngram.transform(filtered)\n",
        "print(\"=== After N-Gram Creation (Bigrams) ===\")\n",
        "ngramDF.select(\"filtered\", \"ngrams\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_r-Q8pm1UXA",
        "outputId": "a6ac90d9-4958-4d7b-cf19-b8d79f442947"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== After N-Gram Creation (Bigrams) ===\n",
            "+---------------------------------------------+-------------------------------------------------------------------------+\n",
            "|filtered                                     |ngrams                                                                   |\n",
            "+---------------------------------------------+-------------------------------------------------------------------------+\n",
            "|[big, data, analytics, spark, hadoop]        |[big data, data analytics, analytics spark, spark hadoop]                |\n",
            "|[python, machine, learning, data, science]   |[python machine, machine learning, learning data, data science]          |\n",
            "|[big, data, tools, like, spark,, hive,, hdfs]|[big data, data tools, tools like, like spark,, spark, hive,, hive, hdfs]|\n",
            "|[deep, learning, using, tensorflow, pytorch] |[deep learning, learning using, using tensorflow, tensorflow pytorch]    |\n",
            "+---------------------------------------------+-------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. CountVectorizer\n"
      ],
      "metadata": {
        "id": "T8wFfT1_1WpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"ngrams\",  # can also use \"filtered\"\n",
        "    outputCol=\"countVec\",\n",
        "    vocabSize=1000,\n",
        "    minDF=1\n",
        ")\n",
        "cvModel = cv.fit(ngramDF)\n",
        "counted = cvModel.transform(ngramDF)\n",
        "vocab = cvModel.vocabulary\n",
        "print(\"=== Vocabulary ===\")\n",
        "for i, term in enumerate(vocab):\n",
        "    print(f\"{i}: {term}\")\n",
        "\n",
        "print(\"=== After CountVectorizer (Sparse Vectors) ===\")\n",
        "counted.select(\"ngrams\", \"countVec\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRHuNOdH1Zms",
        "outputId": "0b4e3ef6-447c-4c6d-f8d7-433da7ee4561"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Vocabulary ===\n",
            "0: big data\n",
            "1: spark, hive,\n",
            "2: like spark,\n",
            "3: using tensorflow\n",
            "4: spark hadoop\n",
            "5: hive, hdfs\n",
            "6: tensorflow pytorch\n",
            "7: python machine\n",
            "8: learning data\n",
            "9: data tools\n",
            "10: learning using\n",
            "11: analytics spark\n",
            "12: data science\n",
            "13: machine learning\n",
            "14: deep learning\n",
            "15: data analytics\n",
            "16: tools like\n",
            "=== After CountVectorizer (Sparse Vectors) ===\n",
            "+-------------------------------------------------------------------------+---------------------------------------------+\n",
            "|ngrams                                                                   |countVec                                     |\n",
            "+-------------------------------------------------------------------------+---------------------------------------------+\n",
            "|[big data, data analytics, analytics spark, spark hadoop]                |(17,[0,4,11,15],[1.0,1.0,1.0,1.0])           |\n",
            "|[python machine, machine learning, learning data, data science]          |(17,[7,8,12,13],[1.0,1.0,1.0,1.0])           |\n",
            "|[big data, data tools, tools like, like spark,, spark, hive,, hive, hdfs]|(17,[0,1,2,5,9,16],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|[deep learning, learning using, using tensorflow, tensorflow pytorch]    |(17,[3,6,10,14],[1.0,1.0,1.0,1.0])           |\n",
            "+-------------------------------------------------------------------------+---------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. ChiSqSelector (Feature Selection)\n"
      ],
      "metadata": {
        "id": "ZrUvofgV1cgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import ChiSqSelector\n",
        "chisq = ChiSqSelector(\n",
        "    featuresCol=\"countVec\",\n",
        "    labelCol=\"label\",\n",
        "    outputCol=\"selectedFeatures\",\n",
        "    numTopFeatures=5\n",
        ")\n",
        "chisq_model = chisq.fit(counted)\n",
        "selected = chisq_model.transform(counted)"
      ],
      "metadata": {
        "id": "XJyAXbYQ1lta"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Viewing select features and their indexes"
      ],
      "metadata": {
        "id": "UQYAbGLW1uXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show transformed data\n",
        "selected.select(\"label\", \"selectedFeatures\").show(truncate=False)\n",
        "\n",
        "selected_indices = chisq_model.selectedFeatures\n",
        "print(\"=== Selected feature indices ===\")\n",
        "print(selected_indices)\n",
        "\n",
        "# Step 6: Print corresponding feature names using vocabulary\n",
        "selected_terms = [vocab[i] for i in selected_indices]\n",
        "print(\"=== Selected feature names ===\")\n",
        "for idx, term in zip(selected_indices, selected_terms):\n",
        "    print(f\"{idx}: {term}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bmMEk513R6",
        "outputId": "b3d5f588-68dc-41ca-a656-f2ef1736ecf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------+\n",
            "|label|selectedFeatures         |\n",
            "+-----+-------------------------+\n",
            "|0    |(5,[0,4],[1.0,1.0])      |\n",
            "|1    |(5,[],[])                |\n",
            "|0    |(5,[0,1,2],[1.0,1.0,1.0])|\n",
            "|1    |(5,[3],[1.0])            |\n",
            "+-----+-------------------------+\n",
            "\n",
            "=== Selected feature indices ===\n",
            "[0, 1, 2, 3, 4]\n",
            "=== Selected feature names ===\n",
            "0: big data\n",
            "1: spark, hive,\n",
            "2: like spark,\n",
            "3: using tensorflow\n",
            "4: spark hadoop\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwROlWvUnILyWmE2cZ2w5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbisgin/BigDatav1/blob/main/Lecture21_MLP_OneHiddenLayerOneNodeBreastCancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy-only MLP with exactly ONE hidden neuron\n",
        "# Dataset: Breast Cancer Wisconsin (binary classification)\n",
        "# Architecture: d -> 1 -> 1  (ReLU hidden, Sigmoid output, BCE loss)\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer  # data loading only\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- 1) Data ----------\n",
        "data = load_breast_cancer()\n",
        "X = data.data.astype(np.float64)                  # (n, d)\n",
        "y = data.target.astype(np.float64).reshape(-1, 1) # (n, 1)  (0/1)\n",
        "\n",
        "# Standardize features\n",
        "mu = X.mean(axis=0, keepdims=True)\n",
        "sd = X.std(axis=0, keepdims=True) + 1e-12\n",
        "X = (X - mu) / sd\n",
        "\n",
        "# Train/test split (80/20)\n",
        "n = X.shape[0]\n",
        "perm = np.random.permutation(n)\n",
        "ntr = int(0.8 * n)\n",
        "tr, te = perm[:ntr], perm[ntr:]\n",
        "Xtr, ytr = X[tr], y[tr]\n",
        "Xte, yte = X[te], y[te]\n",
        "\n",
        "# ---------- 2) Model: d -> 1 -> 1 ----------\n",
        "d = Xtr.shape[1]\n",
        "\n",
        "# ONE hidden neuron: weights (d×1), bias (1×1)\n",
        "W1 = np.random.randn(d, 1) * np.sqrt(2.0 / d)\n",
        "b1 = np.zeros((1, 1))\n",
        "\n",
        "# Output neuron: weights (1×1), bias (1×1)\n",
        "W2 = np.random.randn(1, 1) * np.sqrt(2.0 / 1.0)\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "def relu(z): return np.maximum(0.0, z)\n",
        "def relu_deriv(z): return (z > 0).astype(z.dtype)\n",
        "def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def bce_loss(y_true, y_prob, eps=1e-12):\n",
        "    # manual clamp (no np.clip)\n",
        "    p = np.maximum(eps, np.minimum(1.0 - eps, y_prob))\n",
        "    return -np.mean(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n",
        "\n",
        "def accuracy(y_true, y_prob):\n",
        "    return np.mean((y_prob >= 0.5).astype(np.float64) == y_true)\n",
        "\n",
        "# ---------- 3) Train (full-batch GD) ----------\n",
        "lr = 0.05\n",
        "epochs = 2000\n",
        "wd = 0.0  # L2 weight decay (try 1e-4)\n",
        "\n",
        "for t in range(1, epochs + 1):\n",
        "    # Forward\n",
        "    z1 = np.dot(Xtr, W1) + b1      # (ntr, 1)\n",
        "    a1 = relu(z1)                  # (ntr, 1)\n",
        "    z2 = np.dot(a1, W2) + b2       # (ntr, 1)\n",
        "    p  = sigmoid(z2)               # (ntr, 1)\n",
        "\n",
        "    # Loss (+ optional L2)\n",
        "    loss = bce_loss(ytr, p)\n",
        "    if wd > 0:\n",
        "        loss += 0.5 * wd * (np.sum(W1*W1) + np.sum(W2*W2))\n",
        "\n",
        "    # Backprop (sigmoid+BCE => dL/dz2 = (p - y)/n)\n",
        "    ntr_ = Xtr.shape[0]\n",
        "    dz2 = (p - ytr) / ntr_               # (ntr, 1)\n",
        "    dW2 = np.dot(a1.T, dz2)              # (1, 1)\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    da1 = np.dot(dz2, W2.T)              # (ntr, 1)\n",
        "    dz1 = da1 * relu_deriv(z1)           # (ntr, 1)\n",
        "    dW1 = np.dot(Xtr.T, dz1)             # (d, 1)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    if wd > 0:\n",
        "        dW2 += wd * W2\n",
        "        dW1 += wd * W1\n",
        "\n",
        "    # Gradient step\n",
        "    W2 -= lr * dW2; b2 -= lr * db2\n",
        "    W1 -= lr * dW1; b1 -= lr * db1\n",
        "\n",
        "    # Monitor\n",
        "    if t % 200 == 0 or t == 1:\n",
        "        train_acc = accuracy(ytr, p)\n",
        "        p_te = sigmoid(np.dot(relu(np.dot(Xte, W1) + b1), W2) + b2)\n",
        "        test_loss = bce_loss(yte, p_te)\n",
        "        test_acc = accuracy(yte, p_te)\n",
        "        print(f\"iter {t:4d} | train loss {loss:.4f} acc {train_acc:.3f} | \"\n",
        "              f\"test loss {test_loss:.4f} acc {test_acc:.3f}\")\n",
        "\n",
        "# ---------- 4) Final eval ----------\n",
        "p_tr = sigmoid(np.dot(relu(np.dot(Xtr, W1) + b1), W2) + b2)\n",
        "p_te = sigmoid(np.dot(relu(np.dot(Xte, W1) + b1), W2) + b2)\n",
        "print(\"\\nFinal:\")\n",
        "print(\"Train acc =\", accuracy(ytr, p_tr))\n",
        "print(\"Test  acc =\", accuracy(yte, p_te))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1nWUaoIO3zm",
        "outputId": "755d8d9c-1b14-4453-b5cf-dbb5b362f799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter    1 | train loss 0.7585 acc 0.611 | test loss 0.6942 acc 0.649\n",
            "iter  200 | train loss 0.1973 acc 0.965 | test loss 0.2378 acc 0.947\n",
            "iter  400 | train loss 0.1200 acc 0.978 | test loss 0.1643 acc 0.956\n",
            "iter  600 | train loss 0.0915 acc 0.980 | test loss 0.1370 acc 0.965\n",
            "iter  800 | train loss 0.0779 acc 0.985 | test loss 0.1256 acc 0.965\n",
            "iter 1000 | train loss 0.0699 acc 0.985 | test loss 0.1200 acc 0.965\n",
            "iter 1200 | train loss 0.0647 acc 0.985 | test loss 0.1169 acc 0.965\n",
            "iter 1400 | train loss 0.0610 acc 0.985 | test loss 0.1153 acc 0.974\n",
            "iter 1600 | train loss 0.0582 acc 0.987 | test loss 0.1148 acc 0.974\n",
            "iter 1800 | train loss 0.0559 acc 0.987 | test loss 0.1151 acc 0.974\n",
            "iter 2000 | train loss 0.0541 acc 0.987 | test loss 0.1161 acc 0.974\n",
            "\n",
            "Final:\n",
            "Train acc = 0.9868131868131869\n",
            "Test  acc = 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Below code has multiple nodes in one hidden layer"
      ],
      "metadata": {
        "id": "8Lmf-Z5k3FJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thvEWQbVS2OG",
        "outputId": "1745bf40-a4c4-4dd2-c79d-5437c25236ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter    1 | train loss 0.4410 acc 0.776 | test loss 0.4016 acc 0.798\n",
            "iter  200 | train loss 0.0879 acc 0.978 | test loss 0.1173 acc 0.956\n",
            "iter  400 | train loss 0.0656 acc 0.987 | test loss 0.1076 acc 0.965\n",
            "iter  600 | train loss 0.0556 acc 0.987 | test loss 0.1032 acc 0.965\n",
            "iter  800 | train loss 0.0487 acc 0.989 | test loss 0.1001 acc 0.965\n",
            "iter 1000 | train loss 0.0437 acc 0.993 | test loss 0.0998 acc 0.974\n",
            "iter 1200 | train loss 0.0398 acc 0.993 | test loss 0.1004 acc 0.965\n",
            "iter 1400 | train loss 0.0367 acc 0.993 | test loss 0.1009 acc 0.965\n",
            "iter 1600 | train loss 0.0339 acc 0.993 | test loss 0.1013 acc 0.965\n",
            "iter 1800 | train loss 0.0314 acc 0.993 | test loss 0.1020 acc 0.965\n",
            "iter 2000 | train loss 0.0292 acc 0.993 | test loss 0.1036 acc 0.965\n",
            "\n",
            "Final:\n",
            "Train acc = 0.9934065934065934\n",
            "Test  acc = 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer  # data loading only\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ------------ 1) Data ------------\n",
        "data = load_breast_cancer()\n",
        "X = data.data.astype(np.float64)                  # (n, d)\n",
        "y = data.target.astype(np.float64).reshape(-1,1)  # (n, 1)  (0/1)\n",
        "\n",
        "# standardize features\n",
        "mu = X.mean(axis=0, keepdims=True)\n",
        "sd = X.std(axis=0, keepdims=True) + 1e-12\n",
        "X = (X - mu) / sd\n",
        "\n",
        "# simple train/test split (80/20)\n",
        "n = X.shape[0]\n",
        "perm = np.random.permutation(n)\n",
        "ntr = int(0.8 * n)\n",
        "tr, te = perm[:ntr], perm[ntr:]\n",
        "Xtr, ytr = X[tr], y[tr]\n",
        "Xte, yte = X[te], y[te]\n",
        "\n",
        "# ------------ 2) Model ------------\n",
        "d = Xtr.shape[1]\n",
        "H = 16  # hidden width\n",
        "\n",
        "W1 = np.random.randn(d, H) * np.sqrt(2.0 / d)\n",
        "b1 = np.zeros((1, H))\n",
        "W2 = np.random.randn(H, 1) * np.sqrt(2.0 / H)\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "def relu(z): return np.maximum(0.0, z)\n",
        "def relu_deriv(z): return (z > 0).astype(z.dtype)\n",
        "def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def bce_loss(y_true, y_prob, eps=1e-12):\n",
        "    p = np.maximum(eps, np.minimum(1.0 - eps, y_prob))\n",
        "    return -np.mean(y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n",
        "\n",
        "def accuracy(y_true, y_prob):\n",
        "    return np.mean((y_prob >= 0.5).astype(np.float64) == y_true)\n",
        "\n",
        "# ------------ 3) Train (full-batch GD) ------------\n",
        "lr = 0.05\n",
        "epochs = 2000\n",
        "wd = 0.0  # L2 weight decay (try 1e-4)\n",
        "\n",
        "for t in range(1, epochs+1):\n",
        "    # forward\n",
        "    z1 = np.dot(Xtr, W1) + b1         # (ntr, H)\n",
        "    a1 = relu(z1)                      # (ntr, H)\n",
        "    z2 = np.dot(a1, W2) + b2          # (ntr, 1)\n",
        "    p  = sigmoid(z2)                   # (ntr, 1)\n",
        "\n",
        "    # loss (+ optional L2)\n",
        "    loss = bce_loss(ytr, p)\n",
        "    if wd > 0:\n",
        "        loss += 0.5 * wd * (np.sum(W1*W1) + np.sum(W2*W2))\n",
        "\n",
        "    # backward (sigmoid+BCE => dL/dz2 = (p - y) / n)\n",
        "    ntr_ = Xtr.shape[0]\n",
        "    dz2 = (p - ytr) / ntr_            # (ntr, 1)\n",
        "    dW2 = np.dot(a1.T, dz2)           # (H, 1)\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    da1 = np.dot(dz2, W2.T)           # (ntr, H)\n",
        "    dz1 = da1 * relu_deriv(z1)        # (ntr, H)\n",
        "    dW1 = np.dot(Xtr.T, dz1)          # (d, H)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    if wd > 0:\n",
        "        dW2 += wd * W2\n",
        "        dW1 += wd * W1\n",
        "\n",
        "    # gradient step\n",
        "    W2 -= lr * dW2; b2 -= lr * db2\n",
        "    W1 -= lr * dW1; b1 -= lr * db1\n",
        "\n",
        "    # monitor\n",
        "    if t % 200 == 0 or t == 1:\n",
        "        train_acc = accuracy(ytr, p)\n",
        "        p_te = sigmoid(np.dot(relu(np.dot(Xte, W1) + b1), W2) + b2)\n",
        "        test_loss = bce_loss(yte, p_te)\n",
        "        test_acc = accuracy(yte, p_te)\n",
        "        print(f\"iter {t:4d} | train loss {loss:.4f} acc {train_acc:.3f} | \"\n",
        "              f\"test loss {test_loss:.4f} acc {test_acc:.3f}\")\n",
        "\n",
        "# ------------ 4) Final eval ------------\n",
        "p_tr = sigmoid(np.dot(relu(np.dot(Xtr, W1) + b1), W2) + b2)\n",
        "p_te = sigmoid(np.dot(relu(np.dot(Xte, W1) + b1), W2) + b2)\n",
        "print(\"\\nFinal:\")\n",
        "print(\"Train acc =\", accuracy(ytr, p_tr))\n",
        "print(\"Test  acc =\", accuracy(yte, p_te))\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEXZ9nsJ+Dk0CDyY/i3T7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbisgin/BigDatav1/blob/main/Lecture20_NN_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict petal_width (y) from sepal_length (x1) only\n",
        "# 1) OLS via scikit-learn\n",
        "# 2) Single linear neuron via plain NumPy gradient descent\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "iris = load_iris()\n",
        "x1 = iris.data[:, 0]      # sepal_length\n",
        "y  = iris.data[:, 3]      # petal_width\n",
        "n  = x1.shape[0]\n",
        "\n",
        "# -----------------------------\n",
        "# (1) OLS with scikit-learn\n",
        "# -----------------------------\n",
        "X = x1.reshape(-1, 1)\n",
        "lin = LinearRegression()\n",
        "lin.fit(X, y)\n",
        "\n",
        "b_ols  = float(lin.intercept_)\n",
        "w1_ols = float(lin.coef_[0])\n",
        "\n",
        "y_hat_ols = b_ols + w1_ols * x1\n",
        "mse_ols = np.mean((y_hat_ols - y) ** 2)\n",
        "\n",
        "print(\"OLS (sklearn) coefficients:\")\n",
        "print(f\"  bias={b_ols:.4f}, w1(sepal_len)={w1_ols:.4f}\")\n",
        "print(f\"OLS MSE: {mse_ols:.4f}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# (2) Gradient Descent\n",
        "# Model: y_hat = b + w1*x1\n",
        "# Loss: MSE\n",
        "# -----------------------------\n",
        "rng = np.random.default_rng(42)\n",
        "b  = float(rng.normal(scale=1e-3)) #random number generation\n",
        "w1 = float(rng.normal(scale=1e-3))\n",
        "\n",
        "lr = 1e-3\n",
        "epochs = 20000\n",
        "\n",
        "for t in range(epochs):\n",
        "    # forward\n",
        "    y_hat = b + w1 * x1\n",
        "    err = y_hat - y #I will also use this when I calculate gradients/derivatives\n",
        "\n",
        "    # loss and gradients\n",
        "    mse = np.mean(err ** 2)\n",
        "    db  = (2.0 / n) * np.sum(err)\n",
        "    dw1 = (2.0 / n) * np.sum(err * x1)\n",
        "\n",
        "    # update\n",
        "    b  -= lr * db\n",
        "    w1 -= lr * dw1\n",
        "\n",
        "    if (t + 1) % 5000 == 0:\n",
        "        print(f\"iter {t+1:5d}: MSE={mse:.4f}\")\n",
        "\n",
        "# final metrics\n",
        "y_hat = b + w1 * x1\n",
        "mse_gd = np.mean((y_hat - y) ** 2)\n",
        "\n",
        "print(\"\\nGradient Descent (plain NumPy) coefficients:\")\n",
        "print(f\"  bias={b:.4f}, w1(sepal_len)={w1:.4f}\")\n",
        "print(f\"GD MSE: {mse_gd:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\nDifference (GD - OLS):\")\n",
        "print(f\"  bias={b - b_ols:+.4e}, w1={w1 - w1_ols:+.4e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP0u9y7ideh6",
        "outputId": "b03750a7-0d50-4952-c057-a03325585906"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS (sklearn) coefficients:\n",
            "  bias=-3.2002, w1(sepal_len)=0.7529\n",
            "OLS MSE: 0.1910\n",
            "\n",
            "iter  5000: MSE=0.3310\n",
            "iter 10000: MSE=0.2867\n",
            "iter 15000: MSE=0.2564\n",
            "iter 20000: MSE=0.2357\n",
            "\n",
            "Gradient Descent (plain NumPy) coefficients:\n",
            "  bias=-1.6883, w1(sepal_len)=0.4991\n",
            "GD MSE: 0.2357\n",
            "\n",
            "Difference (GD - OLS):\n",
            "  bias=+1.5120e+00, w1=-2.5383e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1, b)\n",
        "print(w1*x1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9OzIykY_Wmf",
        "outputId": "e30350b8-644c-498c-f772-b6434bd7977f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49909126952753347 -1.68826261227548\n",
            "[2.54536547 2.44554722 2.34572897 2.29581984 2.49545635 2.69509286\n",
            " 2.29581984 2.49545635 2.19600159 2.44554722 2.69509286 2.39563809\n",
            " 2.39563809 2.14609246 2.89472936 2.84482024 2.69509286 2.54536547\n",
            " 2.84482024 2.54536547 2.69509286 2.54536547 2.29581984 2.54536547\n",
            " 2.39563809 2.49545635 2.49545635 2.5952746  2.5952746  2.34572897\n",
            " 2.39563809 2.69509286 2.5952746  2.74500198 2.44554722 2.49545635\n",
            " 2.74500198 2.44554722 2.19600159 2.54536547 2.49545635 2.24591071\n",
            " 2.19600159 2.49545635 2.54536547 2.39563809 2.54536547 2.29581984\n",
            " 2.64518373 2.49545635 3.49363889 3.19418412 3.44372976 2.74500198\n",
            " 3.24409325 2.84482024 3.144275   2.44554722 3.29400238 2.5952746\n",
            " 2.49545635 2.94463849 2.99454762 3.04445674 2.79491111 3.34391151\n",
            " 2.79491111 2.89472936 3.09436587 2.79491111 2.94463849 3.04445674\n",
            " 3.144275   3.04445674 3.19418412 3.29400238 3.39382063 3.34391151\n",
            " 2.99454762 2.84482024 2.74500198 2.74500198 2.89472936 2.99454762\n",
            " 2.69509286 2.99454762 3.34391151 3.144275   2.79491111 2.74500198\n",
            " 2.74500198 3.04445674 2.89472936 2.49545635 2.79491111 2.84482024\n",
            " 2.84482024 3.09436587 2.54536547 2.84482024 3.144275   2.89472936\n",
            " 3.54354801 3.144275   3.24409325 3.79309365 2.44554722 3.64336627\n",
            " 3.34391151 3.59345714 3.24409325 3.19418412 3.39382063 2.84482024\n",
            " 2.89472936 3.19418412 3.24409325 3.84300278 3.84300278 2.99454762\n",
            " 3.44372976 2.79491111 3.84300278 3.144275   3.34391151 3.59345714\n",
            " 3.09436587 3.04445674 3.19418412 3.59345714 3.69327539 3.94282103\n",
            " 3.19418412 3.144275   3.04445674 3.84300278 3.144275   3.19418412\n",
            " 2.99454762 3.44372976 3.34391151 3.44372976 2.89472936 3.39382063\n",
            " 3.34391151 3.34391151 3.144275   3.24409325 3.09436587 2.94463849]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbq0hwefcRYD",
        "outputId": "35a41db5-c24b-4170-bbcd-8f385349fd32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OLS (sklearn) coefficients:\n",
            "  bias=-0.0090, w1(sepal_len)=-0.0822, w2(petal_len)=0.4494\n",
            "OLS MSE: 0.0410\n",
            "\n",
            "iter  5000: MSE=0.0410\n",
            "iter 10000: MSE=0.0410\n",
            "iter 15000: MSE=0.0410\n",
            "iter 20000: MSE=0.0410\n",
            "\n",
            "Gradient Descent (plain NumPy) coefficients:\n",
            "  bias=-0.0402, w1(sepal_len)=-0.0753, w2(petal_len)=0.4468\n",
            "GD MSE: 0.0410\n",
            "\n",
            "Difference (GD - OLS):\n",
            "  bias=-3.1155e-02, w1=+6.9197e-03, w2=-2.5350e-03\n"
          ]
        }
      ],
      "source": [
        "# Predict petal_width (y) from sepal_length (x1) and petal_length (x2)\n",
        "# 1) Ordinary Least Squares (OLS) via sklearn\n",
        "# 2) Single linear neuron via plain NumPy gradient descent\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "iris = load_iris()\n",
        "X = iris.data[:, [0, 2]]   # x1 = sepal_length, x2 = petal_length\n",
        "y = iris.data[:, 3]        # petal_width\n",
        "n = X.shape[0]\n",
        "\n",
        "# -----------------------------\n",
        "# (1) OLS with scikit-learn\n",
        "# -----------------------------\n",
        "lin = LinearRegression()\n",
        "lin.fit(X, y)\n",
        "\n",
        "b_ols  = float(lin.intercept_)\n",
        "w1_ols = float(lin.coef_[0])\n",
        "w2_ols = float(lin.coef_[1])\n",
        "\n",
        "y_hat_ols = b_ols + w1_ols * X[:, 0] + w2_ols * X[:, 1]\n",
        "mse_ols = np.mean((y_hat_ols - y) ** 2)\n",
        "\n",
        "print(\"OLS (sklearn) coefficients:\")\n",
        "print(f\"  bias={b_ols:.4f}, w1(sepal_len)={w1_ols:.4f}, w2(petal_len)={w2_ols:.4f}\")\n",
        "print(f\"OLS MSE: {mse_ols:.4f}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# (2) Single linear neuron via Gradient Descent\n",
        "# Model: y_hat = b + w1*x1 + w2*x2\n",
        "# Loss: MSE\n",
        "# -----------------------------\n",
        "rng = np.random.default_rng(42)\n",
        "b, w1, w2 = rng.normal(scale=1e-3, size=3)  # produces 3 small random values centered around zero, with a tiny spread (std dev = 0.001).\n",
        "\n",
        "lr = 1e-3\n",
        "epochs = 20000\n",
        "\n",
        "for t in range(epochs):\n",
        "    y_hat = b + w1 * X[:, 0] + w2 * X[:, 1]\n",
        "    err = y_hat - y\n",
        "\n",
        "    # MSE and gradients\n",
        "    mse = np.mean(err ** 2)\n",
        "    db  = (2.0 / n) * np.sum(err)\n",
        "    dw1 = (2.0 / n) * np.sum(err * X[:, 0])\n",
        "    dw2 = (2.0 / n) * np.sum(err * X[:, 1])\n",
        "\n",
        "    # update\n",
        "    b  -= lr * db\n",
        "    w1 -= lr * dw1\n",
        "    w2 -= lr * dw2\n",
        "\n",
        "    if (t + 1) % 5000 == 0:\n",
        "        print(f\"iter {t+1:5d}: MSE={mse:.4f}\")\n",
        "\n",
        "# final metrics\n",
        "y_hat = b + w1 * X[:, 0] + w2 * X[:, 1]\n",
        "mse_gd = np.mean((y_hat - y) ** 2)\n",
        "\n",
        "print(\"\\nGradient Descent (plain NumPy) coefficients:\")\n",
        "print(f\"  bias={b:.4f}, w1(sepal_len)={w1:.4f}, w2(petal_len)={w2:.4f}\")\n",
        "print(f\"GD MSE: {mse_gd:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\nDifference (GD - OLS):\")\n",
        "print(f\"  bias={b - b_ols:+.4e}, w1={w1 - w1_ols:+.4e}, w2={w2 - w2_ols:+.4e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression w/ NNs"
      ],
      "metadata": {
        "id": "BWCosPBnhZf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deault LR from sklearn"
      ],
      "metadata": {
        "id": "g_zeKc2Whewb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression on Iris (binary: Virginica vs. not)\n",
        "# Scenario A: ONE feature (x1)\n",
        "#   1) sklearn LogisticRegression\n",
        "#   2) Manual neuron with sigmoid + cross-entropy + explicit gradients (no @)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -----------------------------\n",
        "# Data: x1 only, label y = 1(virginica), 0(other)\n",
        "# -----------------------------\n",
        "iris = load_iris()\n",
        "x1 = iris.data[:, 2]            # petal_length is very separable\n",
        "y  = (iris.target == 2).astype(float)  # virginica=1\n",
        "\n",
        "n = x1.shape[0]\n",
        "\n",
        "# (Optional) standardize x1 for nicer GD convergence\n",
        "x1_mean, x1_std = x1.mean(), x1.std()\n",
        "x1s = (x1 - x1_mean) / (x1_std + 1e-12)\n",
        "\n",
        "# -----------------------------\n",
        "# (1) scikit-learn\n",
        "# -----------------------------\n",
        "X_sklearn = x1s.reshape(-1, 1)\n",
        "clf = LogisticRegression(solver=\"lbfgs\") #this is just an option if you would like to give\n",
        "# scikit-learn solves logistic regression as a convex optimization problem using iterative optimization algorithms.\n",
        "\n",
        "clf.fit(X_sklearn, y)\n",
        "prob_sklearn = clf.predict_proba(X_sklearn)[:, 1]\n",
        "pred_sklearn = (prob_sklearn >= 0.5).astype(float)\n",
        "print(\"[sklearn-1feat] acc =\", accuracy_score(y, pred_sklearn))\n",
        "\n",
        "# -----------------------------\n",
        "# (2) Manual logistic neuron\n",
        "# Model: p = sigmoid(b + w1 * x1s)\n",
        "# Loss: average binary cross-entropy\n",
        "# Gradients:\n",
        "#   db  = (1/n) * sum(p - y)\n",
        "#   dw1 = (1/n) * sum((p - y) * x1s)\n",
        "# -----------------------------\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "b  = float(rng.normal(scale=1e-2))\n",
        "w1 = float(rng.normal(scale=1e-2))\n",
        "\n",
        "lr = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "for t in range(epochs):\n",
        "    z = b + w1 * x1s\n",
        "    p = sigmoid(z)\n",
        "    # cross-entropy (for monitoring)\n",
        "    eps = 1e-12\n",
        "    ce = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
        "    db  = np.mean(p - y)\n",
        "    dw1 = np.mean((p - y) * x1s)\n",
        "    # update\n",
        "    b  -= lr * db\n",
        "    w1 -= lr * dw1\n",
        "    # (optional) print every 500 iters\n",
        "    # if (t+1) % 500 == 0: print(t+1, ce)\n",
        "\n",
        "p_hat = sigmoid(b + w1 * x1s)\n",
        "pred_gd = (p_hat >= 0.5).astype(float)\n",
        "print(\"[manual-1feat]  acc =\", accuracy_score(y, pred_gd))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_psX_JdhY5d",
        "outputId": "77519d2c-3e25-4c58-f786-91fb49b17956"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sklearn-1feat] acc = 0.9533333333333334\n",
            "[manual-1feat]  acc = 0.9533333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the multi-variable scenario"
      ],
      "metadata": {
        "id": "PXQIopajhpBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression on Iris (binary: Virginica vs. not)\n",
        "# Scenario B: TWO features (x1, x2)\n",
        "#   1) sklearn LogisticRegression\n",
        "#   2) Manual neuron with sigmoid + cross-entropy + explicit gradients (no @)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -----------------------------\n",
        "# Data: choose two good features\n",
        "# -----------------------------\n",
        "iris = load_iris()\n",
        "x1 = iris.data[:, 2]            # petal_length\n",
        "x2 = iris.data[:, 3]            # petal_width\n",
        "y  = (iris.target == 2).astype(float)  # virginica=1\n",
        "\n",
        "n = x1.shape[0]\n",
        "\n",
        "# Standardize features for smoother GD\n",
        "x1s = (x1 - x1.mean()) / (x1.std() + 1e-12)\n",
        "x2s = (x2 - x2.mean()) / (x2.std() + 1e-12)\n",
        "\n",
        "# -----------------------------\n",
        "# (1) scikit-learn\n",
        "# -----------------------------\n",
        "X_sklearn = np.column_stack([x1s, x2s])  # just for sklearn\n",
        "clf = LogisticRegression(solver=\"lbfgs\")\n",
        "clf.fit(X_sklearn, y)\n",
        "prob_sklearn = clf.predict_proba(X_sklearn)[:, 1]\n",
        "pred_sklearn = (prob_sklearn >= 0.5).astype(float)\n",
        "print(\"[sklearn-2feat] acc =\", accuracy_score(y, pred_sklearn))\n",
        "\n",
        "# -----------------------------\n",
        "# (2) Manual logistic neuron (no @)\n",
        "# Model: p = sigmoid(b + w1*x1s + w2*x2s)\n",
        "# Loss: average binary cross-entropy\n",
        "# Gradients:\n",
        "#   db  = (1/n) * sum(p - y)\n",
        "#   dw1 = (1/n) * sum((p - y) * x1s)\n",
        "#   dw2 = (1/n) * sum((p - y) * x2s)\n",
        "# -----------------------------\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "rng = np.random.default_rng(1)\n",
        "b  = float(rng.normal(scale=1e-2))\n",
        "w1 = float(rng.normal(scale=1e-2))\n",
        "w2 = float(rng.normal(scale=1e-2))\n",
        "\n",
        "lr = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "for t in range(epochs):\n",
        "    z = b + w1 * x1s + w2 * x2s\n",
        "    p = sigmoid(z)\n",
        "    # cross-entropy (monitor)\n",
        "    eps = 1e-12\n",
        "    ce = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
        "    # gradients (explicit)\n",
        "    diff = (p - y)\n",
        "    db  = np.mean(diff)\n",
        "    dw1 = np.mean(diff * x1s)\n",
        "    dw2 = np.mean(diff * x2s)\n",
        "    # update\n",
        "    b  -= lr * db\n",
        "    w1 -= lr * dw1\n",
        "    w2 -= lr * dw2\n",
        "    # (optional) print every 500 iters\n",
        "    # if (t+1) % 500 == 0: print(t+1, ce)\n",
        "\n",
        "p_hat = sigmoid(b + w1 * x1s + w2 * x2s)\n",
        "pred_gd = (p_hat >= 0.5).astype(float)\n",
        "print(\"[manual-2feat]  acc =\", accuracy_score(y, pred_gd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQjGnitrhsYV",
        "outputId": "775f9b23-696c-494e-b5a2-b49c8fb75ab6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sklearn-2feat] acc = 0.9533333333333334\n",
            "[manual-2feat]  acc = 0.9533333333333334\n"
          ]
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e15aa09b-4c21-49c0-b9ac-0a416d32f270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Prepare the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3685ac75-4dea-47bc-9d60-a054e6b786c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"ID\", T.LongType(), True),\n",
    "    T.StructField(\"pkgname\", T.StringType(), True),\n",
    "    T.StructField(\"DevRegisteredDomain\", T.LongType(), True),\n",
    "    T.StructField(\"LenDescription\", T.LongType(), True),\n",
    "    T.StructField(\"LenWhatsNew\", T.LongType(), True),\n",
    "    T.StructField(\"ReviewsAverage\", T.DoubleType(), True),\n",
    "    T.StructField(\"CurrentVersion\", T.StringType(), True),\n",
    "    T.StructField(\"Genre\", T.StringType(), True),\n",
    "    T.StructField(\"ContentRating\", T.StringType(), True),\n",
    "    T.StructField(\"LastUpdated\", T.LongType(), True),\n",
    "    T.StructField(\"LenTitle\", T.LongType(), True),\n",
    "    T.StructField(\"AndroidVersion\", T.StringType(), True),\n",
    "    T.StructField(\"DeveloperCategory\", T.StringType(), True),\n",
    "    T.StructField(\"isSpamming\", T.LongType(), True),\n",
    "    T.StructField(\"net\", T.LongType(), True),\n",
    "    T.StructField(\"intent\", T.LongType(), True),\n",
    "    T.StructField(\"bluetooth\", T.LongType(), True),\n",
    "    T.StructField(\"app\", T.LongType(), True),\n",
    "    T.StructField(\"provider\", T.LongType(), True),\n",
    "    T.StructField(\"speech\", T.LongType(), True),\n",
    "    T.StructField(\"nfc\", T.LongType(), True),\n",
    "    T.StructField(\"media\", T.LongType(), True),\n",
    "    T.StructField(\"hardware\", T.LongType(), True),\n",
    "    T.StructField(\"google\", T.LongType(), True),\n",
    "    T.StructField(\"os\", T.LongType(), True),\n",
    "    T.StructField(\"CALENDAR\", T.LongType(), True),\n",
    "    T.StructField(\"CAMERA\", T.LongType(), True),\n",
    "    T.StructField(\"CONTACTS\", T.LongType(), True),\n",
    "    T.StructField(\"LOCATION\", T.LongType(), True),\n",
    "    T.StructField(\"MICROPHONE\", T.LongType(), True),\n",
    "    T.StructField(\"PHONE\", T.LongType(), True),\n",
    "    T.StructField(\"SENSORS\", T.LongType(), True),\n",
    "    T.StructField(\"SMS\", T.LongType(), True),\n",
    "    T.StructField(\"STORAGE\", T.LongType(), True),\n",
    "    T.StructField(\"status\", T.LongType(), True),\n",
    "    T.StructField(\"FourStarRatings\", T.DoubleType(), True),\n",
    "    T.StructField(\"ThreeStarRatings\", T.DoubleType(), True),\n",
    "    T.StructField(\"FiveStarRatings\", T.DoubleType(), True),\n",
    "    T.StructField(\"OneStarRatings\", T.DoubleType(), True),\n",
    "    T.StructField(\"TwoStarRatings\", T.DoubleType(), True),\n",
    "    T.StructField(\"lowest_android_version\", T.StringType(), True),\n",
    "    T.StructField(\"highest_android_version\", T.StringType(), True),\n",
    "    T.StructField(\"paid\", T.LongType(), True),\n",
    "    T.StructField(\"file_size\", T.LongType(), True),\n",
    "    T.StructField(\"max_downloads_log\", T.DoubleType(), True),\n",
    "    T.StructField(\"developer_email\", T.LongType(), True),\n",
    "    T.StructField(\"privacy_policy_link\", T.LongType(), True),\n",
    "    T.StructField(\"developer_address\", T.LongType(), True),\n",
    "    T.StructField(\"developer_website\", T.LongType(), True),\n",
    "    T.StructField(\"days_since_last_update\", T.LongType(), True),\n",
    "    T.StructField(\"malicious_count\", T.LongType(), True),\n",
    "    T.StructField(\"undetected_count\", T.LongType(), True),\n",
    "    T.StructField(\"certificate_life_days\", T.LongType(), True),\n",
    "    T.StructField(\"file_duration_days\", T.LongType(), True),\n",
    "    T.StructField(\"times_submitted\", T.LongType(), True),\n",
    "    T.StructField(\"threat_level\", T.StringType(), True),\n",
    "    T.StructField(\"aggregated_risk_score\", T.LongType(), True),\n",
    "    T.StructField(\"weighted_conf_sum\", T.DoubleType(), True),\n",
    "    T.StructField(\"permission_n\", T.LongType(), True),\n",
    "    T.StructField(\"avg_weight\", T.DoubleType(), True),\n",
    "    T.StructField(\"verdict\", T.StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ce24f5a-564e-418b-b1c4-4feef0b72fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737f466e-122f-43ff-b741-ba9e2202d0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dbfs:/FileStore/BDA_Datasets/val_dataset_t1.csv\", schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce02f1a7-33b1-48ac-9327-71e968099107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Import necessary libraries and perform preprocessing steps including normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b326dd37-8707-4f24-9379-39b2450d598b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "# ---------------------------\n",
    "# Label\n",
    "# ---------------------------\n",
    "label_col = \"verdict\"\n",
    "\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=label_col,\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Numeric feature list\n",
    "# ---------------------------\n",
    "numeric_features = [\n",
    "    \"DevRegisteredDomain\", \"LenDescription\", \"LenWhatsNew\", \"LenTitle\",\n",
    "    \"LastUpdated\", \"days_since_last_update\", \"file_size\", \"max_downloads_log\",\n",
    "    \"file_duration_days\", \"certificate_life_days\", \"times_submitted\",\n",
    "    \"aggregated_risk_score\", \"weighted_conf_sum\", \"avg_weight\",\n",
    "    \"malicious_count\", \"undetected_count\", \"FourStarRatings\",\n",
    "    \"ThreeStarRatings\", \"FiveStarRatings\", \"OneStarRatings\",\n",
    "    \"TwoStarRatings\", \"paid\", \"permission_n\",\n",
    "\n",
    "    # Permission features\n",
    "    \"net\", \"intent\", \"bluetooth\", \"app\", \"provider\", \"speech\", \"nfc\",\n",
    "    \"media\", \"hardware\", \"google\", \"os\", \"CALENDAR\", \"CAMERA\",\n",
    "    \"CONTACTS\", \"LOCATION\", \"MICROPHONE\", \"PHONE\", \"SENSORS\",\n",
    "    \"SMS\", \"STORAGE\"\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# Vector Assembler\n",
    "# ---------------------------\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_features,\n",
    "    outputCol=\"features_raw\", handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Standard Scaler\n",
    "# ---------------------------\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=False,  # MLPC requirement\n",
    "    withStd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af00244-83b7-47b0-ab89-3c350c93450e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# MLP Classifier\n",
    "# ---------------------------\n",
    "input_dim = len(numeric_features)\n",
    "\n",
    "# Determine number of label classes automatically later\n",
    "# but you can also specify manually if needed.\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    layers=[input_dim, 16, 16, 2],  # adapt output size if >2 classes\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Pipeline\n",
    "# ---------------------------\n",
    "pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    mlp\n",
    "])\n",
    "\n",
    "preprocessor_pipeline = Pipeline(stages=[label_indexer,\n",
    "    assembler,\n",
    "    scaler]) # for a later experiment. FYI. \n",
    "    \n",
    "\n",
    "# ---------------------------\n",
    "# Fit model\n",
    "# ---------------------------\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# ---------------------------\n",
    "# Predict\n",
    "# ---------------------------\n",
    "predictions = model.transform(df)\n",
    "predictions.select(\"verdict\", \"label\", \"prediction\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c56b57-6d85-4282-b38a-afb32a65edd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g \" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e1651b-8576-4180-8128-00166ef56141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#What about parameter tuning through CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddec5e5d-e314-4e2a-a607-3034b291cc61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "#before the following steps, I need to make sure I work with the preprocessed data to avoid multiple runs of preprocessing. \n",
    "\n",
    "transformedData = preprocessor_pipeline.fit(df).transform(df)\n",
    "\n",
    "output_classes = 2 \n",
    "\n",
    "# Define layer configurations to test\n",
    "layer_options = [\n",
    "    # Option 1: One wider hidden layer (e.g., 32 nodes)\n",
    "    [input_dim, 32, output_classes],\n",
    "    # Option 2: Two narrower hidden layers  \n",
    "    [input_dim, 16, 16, output_classes],\n",
    "    # Option 3: Two wider hidden layers (e.g., 32 nodes)\n",
    "    [input_dim, 32, 32, output_classes],\n",
    "]\n",
    "\n",
    "# Create the parameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(mlp.layers, layer_options) # Test different layer structures\n",
    "    .addGrid(mlp.maxIter, [50, 100])    \n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf189a0-c1ee-423d-9672-43a34d3e76cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # Or BinaryClassificationEvaluator\n",
    "\n",
    "# Create an Evaluator (e.g., using F1 score for classification)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\" \n",
    ")\n",
    "\n",
    "# Create a simple Pipeline with just the MLP model\n",
    "pipeline = Pipeline(stages=[mlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276e331f-4469-4d71-9e32-6299b2f9cc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Create the CrossValidator instance\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=evaluator, \n",
    "    numFolds=5, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Fit the CrossValidator to your training data\n",
    "# This step will train and evaluate the model for every combination\n",
    "# (number of combinations * numFolds) times.\n",
    "cvModel = cv.fit(transformedData)\n",
    "\n",
    "# The best model found is automatically selected\n",
    "best_model = cvModel.bestModel"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lec21_MLP_on_Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
